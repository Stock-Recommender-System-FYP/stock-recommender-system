{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools as it\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport itertools as it\nimport matplotlib.pyplot as plt\nimport os\nimport operator\nimport pandas as pd\nimport random\n\nfrom tqdm.auto import trange\nfrom cornac.models.recommender import Recommender\nfrom cornac.utils.common import scale\nfrom cornac.exception import ScoreException\nfrom cornac.data import Dataset\nfrom recommenders.datasets.python_splitters import python_random_split\nfrom recommenders.evaluation.python_evaluation import ndcg_at_k\nfrom recommenders.models.cornac.cornac_utils import predict_ranking\nfrom recommenders.utils.timer import Timer\nfrom recommenders.utils.constants import SEED\n\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"id":"1HtSQmz6NwiT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collaborative filtering Approach","metadata":{"id":"NiCbQ0potegr"}},{"cell_type":"code","source":"#@title Default title text\n\nEPS = 1e-10\n\nACT = {\n    \"sigmoid\": nn.Sigmoid(),\n    \"tanh\": nn.Tanh(),\n    \"elu\": nn.ELU(),\n    \"relu\": nn.ReLU(),\n    \"relu6\": nn.ReLU6(),\n}\n\n\nclass BiVAE(nn.Module):\n    def __init__(\n        self,\n        k,\n        user_encoder_structure,\n        item_encoder_structure,\n        act_fn,\n        likelihood,\n        cap_priors,\n        feature_dim,\n        user_batch_size,\n        item_batch_size,\n    ):\n        super(BiVAE, self).__init__()\n\n\n        #initializes mu_theta, mu_beta, theta and beta tensors\n        self.mu_theta = torch.zeros((item_encoder_structure[0], k))  # n_users*k\n        self.mu_beta = torch.zeros((user_encoder_structure[0], k))  # n_items*k\n\n        self.theta = torch.randn(item_encoder_structure[0], k) * 0.01\n        self.beta = torch.randn(user_encoder_structure[0], k) * 0.01\n        torch.nn.init.kaiming_uniform_(self.theta, a=np.sqrt(5))\n\n        self.likelihood = likelihood\n        self.act_fn = ACT.get(act_fn, None)\n        if self.act_fn is None:\n            raise ValueError(\"Supported act_fn: {}\".format(ACT.keys()))\n\n        self.cap_priors = cap_priors\n        if self.cap_priors.get(\"user\", False):\n            self.user_prior_encoder = nn.Linear(feature_dim.get(\"user\"), k)\n        if self.cap_priors.get(\"item\", False):\n            self.item_prior_encoder = nn.Linear(feature_dim.get(\"item\"), k)\n\n        # User Encoder\n        self.user_encoder = nn.Sequential()\n        for i in range(len(user_encoder_structure) - 1):\n            self.user_encoder.add_module(\n                \"fc{}\".format(i),\n                nn.Linear(user_encoder_structure[i], user_encoder_structure[i + 1]),\n            )\n            self.user_encoder.add_module(\"act{}\".format(i), self.act_fn)\n\n            # Apply Xavier initialization to the weights of the linear layer\n            if isinstance(self.user_encoder[-1], nn.Linear):\n                init.xavier_uniform_(self.user_encoder[-1].weight)\n                if self.user_encoder[-1].bias is not None:\n                    init.constant_(self.user_encoder[-1].bias, 0.0)\n        self.user_mu = nn.Linear(user_encoder_structure[-1],k)  # mu\n        self.user_std = nn.Linear(user_encoder_structure[-1],k)\n\n\n        # Item Encoder\n        self.item_encoder = nn.Sequential()\n        for i in range(len(item_encoder_structure) - 1):\n            self.item_encoder.add_module(\n                \"fc{}\".format(i),\n                nn.Linear(item_encoder_structure[i], item_encoder_structure[i + 1]),\n            )\n            self.item_encoder.add_module(\"act{}\".format(i), self.act_fn)\n            if isinstance(self.item_encoder[-1], nn.Linear):\n                init.kaiming_uniform_(self.item_encoder[-1].weight, mode='fan_in', nonlinearity='relu')\n                if self.item_encoder[-1].bias is not None:\n                    init.constant_(self.item_encoder[-1].bias, 0.0)\n\n        self.item_mu = nn.Linear(item_encoder_structure[-1],k)  # mu\n        self.item_std = nn.Linear(item_encoder_structure[-1],k)\n\n    def to(self, device):\n        self.beta = self.beta.to(device=device)\n        self.theta = self.theta.to(device=device)\n        self.mu_beta = self.mu_beta.to(device=device)\n        self.mu_theta = self.mu_theta.to(device=device)\n        return super(BiVAE, self).to(device)\n\n    def encode_user_prior(self, x):\n        h = self.user_prior_encoder(x)\n        return h\n\n    def encode_item_prior(self, x):\n        h = self.item_prior_encoder(x)\n        return h\n\n    def encode_user(self, x):\n        h = self.user_encoder(x)\n        return self.user_mu(h), torch.sigmoid(self.user_std(h))\n\n    def encode_item(self, x):\n        h = self.item_encoder(x)\n        return self.item_mu(h), torch.sigmoid(self.item_std(h))\n\n    def decode_user(self, theta, beta):\n        h = theta.mm(beta.t())\n        return torch.sigmoid(h)\n\n    def decode_item(self, theta, beta):\n        h = beta.mm(theta.t())\n        return torch.sigmoid(h)\n\n    def reparameterize(self, mu, std):\n        eps = torch.randn_like(mu)\n        return mu + eps * std\n\n    def forward(self, x, user=True, beta=None, theta=None):\n\n        if user:\n            mu, std = self.encode_user(x)\n            theta = self.reparameterize(mu, std)\n            return theta, self.decode_user(theta, beta), mu, std\n        else:\n            mu, std = self.encode_item(x)\n            beta = self.reparameterize(mu, std)\n            return beta, self.decode_item(theta, beta), mu, std\n\n    def loss(self, x, x_, mu, mu_prior, std, kl_beta, labels, relevance_scores):\n        # Likelihood\n        ll_choices = {\n            \"bern\": x * torch.log(x_ + EPS) + (1 - x) * torch.log(1 - x_ + EPS),\n            \"gaus\": -(x - x_) ** 2,\n            \"pois\": x * torch.log(x_ + EPS) - x_,\n        }\n\n        ll = ll_choices.get(self.likelihood, None)\n        if ll is None:\n            raise ValueError(\"Supported likelihoods: {}\".format(ll_choices.keys()))\n\n        ll = torch.sum(ll, dim=1)\n\n        # KL term\n        kld = -0.5 * (1 + 2 * torch.log(std) - (mu - mu_prior).pow(2) - std.pow(2))\n        kld = torch.sum(kld, dim=1)\n        \n        return torch.mean(kl_beta * kld - ll) ","metadata":{"id":"6BNCFwYWV4j2","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Default title text\n\ndef learn(\n    bivae,\n    train_set,\n    n_epochs,\n    user_batch_size,\n    item_batch_size,\n    user_learn_rate,\n    item_learn_rate,\n    beta_kl,\n    verbose,\n    plot_loss = False,\n    device=torch.device(\"cpu\"),\n    dtype=torch.float32,\n):\n    user_params = it.chain(\n        bivae.user_encoder.parameters(),\n        bivae.user_mu.parameters(),\n        bivae.user_std.parameters(),\n    )\n\n    item_params = it.chain(\n        bivae.item_encoder.parameters(),\n        bivae.item_mu.parameters(),\n        bivae.item_std.parameters(),\n    )\n\n    if bivae.cap_priors.get(\"user\", False):\n        user_params = it.chain(user_params, bivae.user_prior_encoder.parameters())\n        user_features = train_set.user_feature.features[: train_set.num_users]\n\n    if bivae.cap_priors.get(\"item\", False):\n        item_params = it.chain(item_params, bivae.item_prior_encoder.parameters())\n        item_features = train_set.item_feature.features[: train_set.num_items]\n\n    u_optimizer = torch.optim.Adam(params=user_params, lr=user_learn_rate)\n    i_optimizer = torch.optim.Adam(params=item_params, lr=item_learn_rate)\n\n    x = train_set.matrix.copy()\n    x.data = np.ones_like(x.data)  # Binarize data\n    tx = x.transpose()\n\n    progress_bar = trange(1, n_epochs + 1, disable=not verbose)\n\n    u_loss_list = []\n    i_loss_list = []\n    for _ in progress_bar:\n\n        # item side\n        i_sum_loss = 0.0\n        i_count = 0\n        for i_ids in train_set.item_iter(item_batch_size, shuffle=False):\n            i_batch = tx[i_ids, :]\n            i_batch = i_batch.A\n            i_batch = torch.tensor(i_batch, dtype=dtype, device=device)\n\n            # Reconstructed batch\n            beta, i_batch_, i_mu, i_std = bivae(i_batch, user=False, theta=bivae.theta)\n\n            i_mu_prior = 0.0  # zero mean for standard normal prior if not CAP prior\n            if bivae.cap_priors.get(\"item\", False):\n                i_batch_f = item_features[i_ids]\n                i_batch_f = torch.tensor(i_batch_f, dtype=dtype, device=device)\n                i_mu_prior = bivae.encode_item_prior(i_batch_f)\n\n            i_loss = bivae.loss(i_batch, i_batch_, i_mu, i_mu_prior, i_std, beta_kl, i_batch, beta)\n            i_optimizer.zero_grad()\n            i_loss.backward()\n            i_optimizer.step()\n\n            i_sum_loss += i_loss.data.item()\n            i_count += len(i_batch)\n\n            beta, _, i_mu, _ = bivae(i_batch, user=False, theta=bivae.theta)\n\n            bivae.beta.data[i_ids] = beta.data\n            bivae.mu_beta.data[i_ids] = i_mu.data\n\n\n        # user side\n        u_sum_loss = 0.0\n        u_count = 0\n        for u_ids in train_set.user_iter(user_batch_size, shuffle=False):\n            u_batch = x[u_ids, :]\n            u_batch = u_batch.A\n            u_batch = torch.tensor(u_batch, dtype=dtype, device=device)\n\n            # Reconstructed batch\n            theta, u_batch_, u_mu, u_std = bivae(u_batch, user=True, beta=bivae.beta)\n\n            u_mu_prior = 0.0  # zero mean for standard normal prior if not CAP prior\n            if bivae.cap_priors.get(\"user\", False):\n                u_batch_f = user_features[u_ids]\n                u_batch_f = torch.tensor(u_batch_f, dtype=dtype, device=device)\n                u_mu_prior = bivae.encode_user_prior(u_batch_f)\n\n            u_loss = bivae.loss(u_batch, u_batch_, u_mu, u_mu_prior, u_std, beta_kl, u_batch, theta)\n            u_optimizer.zero_grad()\n            u_loss.backward()\n            u_optimizer.step()\n\n            u_sum_loss += u_loss.data.item()\n            u_count += len(u_batch)\n\n            theta, _, u_mu, _ = bivae(u_batch, user=True, beta=bivae.beta)\n            bivae.theta.data[u_ids] = theta.data\n            bivae.mu_theta.data[u_ids] = u_mu.data\n\n        progress_bar.set_postfix(\n            loss_i=(i_sum_loss / i_count), loss_u=(u_sum_loss / (u_count))\n        )\n        \n        u_loss_list.append(u_sum_loss / u_count)\n        \n        i_loss_list.append(i_sum_loss / i_count)\n\n\n    \n\n    # infer mu_beta\n    for i_ids in train_set.item_iter(item_batch_size, shuffle=False):\n        i_batch = tx[i_ids, :]\n        i_batch = i_batch.A\n        i_batch = torch.tensor(i_batch, dtype=dtype, device=device)\n\n        beta, _, i_mu, _ = bivae(i_batch, user=False, theta=bivae.theta)\n        bivae.mu_beta.data[i_ids] = i_mu.data\n\n    # infer mu_theta\n    for u_ids in train_set.user_iter(user_batch_size, shuffle=False):\n        u_batch = x[u_ids, :]\n        u_batch = u_batch.A\n        u_batch = torch.tensor(u_batch, dtype=dtype, device=device)\n\n        theta, _, u_mu, _ = bivae(u_batch, user=True, beta=bivae.beta)\n        bivae.mu_theta.data[u_ids] = u_mu.data\n\n    #plotting losses\n    x = list(range(1, n_epochs+1))\n\n    if plot_loss:\n        plt.figure(figsize=(10, 5))\n        plt.plot(x, u_loss_list, label = 'u_loss')\n        plt.plot(x, i_loss_list, label = 'i_loss')\n        plt.legend()\n        plt.show()\n    return bivae","metadata":{"id":"h_qOW0igV_qB","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Creating Recommender","metadata":{"id":"oxLK4GoxUUnV"}},{"cell_type":"code","source":"#@title Default title text\n\nclass BiVAECF(Recommender):\n    \"\"\"Bilateral Variational AutoEncoder for Collaborative Filtering.\n    Parameters\n    ----------\n    k: int, optional, default: 10\n        The dimension of the stochastic user ``theta'' and item ``beta'' factors.\n    encoder_structure: list, default: [20]\n        The number of neurons per layer of the user and item encoders for BiVAE.\n        For example, encoder_structure = [20], the user (item) encoder structure will be [num_items, 20, k] ([num_users, 20, k]).\n    act_fn: str, default: 'tanh'\n        Name of the activation function used between hidden layers of the auto-encoder.\n        Supported functions: ['sigmoid', 'tanh', 'elu', 'relu', 'relu6']\n    likelihood: str, default: 'pois'\n        The likelihood function used for modeling the observations.\n        Supported choices:\n        bern: Bernoulli likelihood\n        gaus: Gaussian likelihood\n        pois: Poisson likelihood\n    n_epochs: int, optional, default: 100\n        The number of epochs for SGD.\n    batch_size: int, optional, default: 100\n        The batch size.\n    learning_rate: float, optional, default: 0.001\n        The learning rate for Adam.\n    beta_kl: float, optional, default: 1.0\n        The weight of the KL terms as in beta-VAE.\n    cap_priors: dict, optional, default: {\"user\":False, \"item\":False}\n        When {\"user\":True, \"item\":True}, CAP priors are used (see BiVAE paper for details),\\\n        otherwise the standard Normal is used as a Prior over the user and item latent variables.\n    name: string, optional, default: 'BiVAECF'\n        The name of the recommender model.\n    trainable: boolean, optional, default: True\n        When False, the model is not trained and Cornac assumes that the model is already \\\n        pre-trained.\n    verbose: boolean, optional, default: False\n        When True, some running logs are displayed.\n    seed: int, optional, default: None\n        Random seed for parameters initialization.\n    use_gpu: boolean, optional, default: True\n        If True and your system supports CUDA then training is performed on GPUs.\n    References\n    ----------\n    * Quoc-Tuan Truong, Aghiles Salah, Hady W. Lauw. \" Bilateral Variational Autoencoder for Collaborative Filtering.\"\n    ACM International Conference on Web Search and Data Mining (WSDM). 2021.\n    \"\"\"\n\n    def __init__(\n        self,\n        name=\"BiVAECF\",\n        k=10,\n        user_encoder_structure=[20],\n        item_encoder_structure = [20],\n        act_fn=\"tanh\",\n        likelihood=\"pois\",\n        n_epochs=100,\n        user_batch_size=100,\n        item_batch_size = 100,\n        user_learning_rate=0.001,\n        item_learning_rate=0.001,\n        beta_kl=1.0,\n        cap_priors={\"user\": False, \"item\": False},\n        trainable=True,\n        verbose=False,\n        seed=None,\n        use_gpu=True,\n        plot_loss = False,\n    ):\n        Recommender.__init__(self, name=name, trainable=trainable, verbose=verbose)\n        self.k = k\n        self.user_encoder_structure = user_encoder_structure\n        self.item_encoder_structure = item_encoder_structure\n        self.act_fn = act_fn\n        self.likelihood = likelihood\n        self.user_batch_size = user_batch_size\n        self.item_batch_size = item_batch_size\n        self.n_epochs = n_epochs\n        self.user_learning_rate = user_learning_rate\n        self.item_learning_rate = item_learning_rate\n        self.beta_kl = beta_kl\n        self.cap_priors = cap_priors\n        self.seed = seed\n        self.use_gpu = use_gpu\n        self.plot_loss = plot_loss\n\n    def fit(self, train_set, val_set=None):\n        \"\"\"Fit the model to observations.\n        Parameters\n        ----------\n        train_set: :obj:`cornac.data.Dataset`, required\n            User-Item preference data as well as additional modalities.\n        val_set: :obj:`cornac.data.Dataset`, optional, default: None\n            User-Item preference data for model selection purposes (e.g., early stopping).\n        Returns\n        -------\n        self : object\n        \"\"\"\n        Recommender.fit(self, train_set, val_set)\n\n        import torch\n        # from .bivae import BiVAE, learn\n\n        self.device = (\n            torch.device(\"cuda:0\")\n            if (self.use_gpu and torch.cuda.is_available())\n            else torch.device(\"cpu\")\n        )\n\n        if self.trainable:\n            feature_dim = {\"user\": None, \"item\": None}\n            if self.cap_priors.get(\"user\", False):\n                if train_set.user_feature is None:\n                    raise ValueError(\n                        \"CAP priors for users is set to True but no user features are provided\"\n                    )\n                else:\n                    feature_dim[\"user\"] = train_set.user_feature.feature_dim\n\n            if self.cap_priors.get(\"item\", False):\n                if train_set.item_feature is None:\n                    raise ValueError(\n                        \"CAP priors for items is set to True but no item features are provided\"\n                    )\n                else:\n                    feature_dim[\"item\"] = train_set.item_feature.feature_dim\n\n            if self.seed is not None:\n                torch.manual_seed(self.seed)\n                torch.cuda.manual_seed(self.seed)\n\n            if not hasattr(self, \"bivaecf\"):\n                num_items = train_set.matrix.shape[1]\n                num_users = train_set.matrix.shape[0]\n                self.bivae = BiVAE(\n                    k=self.k,\n                    #changes\n                    user_encoder_structure=[num_items] + self.user_encoder_structure,\n                    item_encoder_structure=[num_users] + self.item_encoder_structure,\n                    #changes end\n                    act_fn=self.act_fn,\n                    likelihood=self.likelihood,\n                    cap_priors=self.cap_priors,\n                    feature_dim=feature_dim,\n                    user_batch_size=self.user_batch_size,\n                    item_batch_size = self.item_batch_size\n                ).to(self.device)\n\n            learn(\n                self.bivae,\n                self.train_set,\n                n_epochs=self.n_epochs,\n                user_batch_size=self.user_batch_size,\n                item_batch_size = self.user_batch_size,\n                user_learn_rate=self.user_learning_rate,\n                item_learn_rate = self.item_learning_rate,\n                beta_kl=self.beta_kl,\n                verbose=self.verbose,\n                device=self.device,\n                plot_loss = self.plot_loss,\n            )\n\n        elif self.verbose:\n            print(\"%s is trained already (trainable = False)\" % (self.name))\n\n        return self\n\n    def score(self, user_idx, item_idx=None):\n        \"\"\"Predict the scores/ratings of a user for an item.\n        Parameters\n        ----------\n        user_idx: int, required\n            The index of the user for whom to perform score prediction.\n        item_idx: int, optional, default: None\n            The index of the item for which to perform score prediction.\n            If None, scores for all known items will be returned.\n        Returns\n        -------\n        res : A scalar or a Numpy array\n            Relative scores that the user gives to the item or to all known items\n        \"\"\"\n\n        if item_idx is None:\n            if self.train_set.is_unk_user(user_idx):\n                raise ScoreException(\n                    \"Can't make score prediction for (user_id=%d)\" % user_idx\n                )\n\n            theta_u = self.bivae.mu_theta[user_idx].view(1, -1)\n            beta = self.bivae.mu_beta\n            known_item_scores = (\n                self.bivae.decode_user(theta_u, beta).cpu().numpy().ravel()\n            )\n\n            return known_item_scores\n        else:\n            if self.train_set.is_unk_user(user_idx) or self.train_set.is_unk_item(\n                item_idx\n            ):\n                raise ScoreException(\n                    \"Can't make score prediction for (user_id=%d, item_id=%d)\"\n                    % (user_idx, item_idx)\n                )\n            theta_u = self.bivae.mu_theta[user_idx].view(1, -1)\n            beta_i = self.bivae.mu_beta[item_idx].view(1, -1)\n            pred = self.bivae.decode_user(theta_u, beta_i).cpu().numpy().ravel()\n\n            pred = scale(\n                pred, self.train_set.min_rating, self.train_set.max_rating, 0.0, 1.0\n            )\n\n            return pred","metadata":{"id":"GhpVvWD0WAbQ","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/user-data/'\ndata_file = os.path.join(file_path,\"atrad_user_history.csv\")\n\ndf = pd.read_csv(data_file, names=[\"userID\", \"itemID\"],dtype = {'userID':np.int32}, skiprows=1)\ndf['itemID'] = df['itemID'].apply(lambda x : x.split(\".\")[0])\ndf = df.assign(rating=1)\ndf.head(2)","metadata":{"id":"SbsyAg-t8l-U","outputId":"678a843e-b35a-4de1-c5bf-3c2a7ba7498c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()\ndf_all = df.copy()","metadata":{"id":"c6UAvGCPdEOM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_dataset(dataframe, min_items, min_interactions):\n    dataframe = dataframe.groupby(\"itemID\").filter(lambda x: len(x) >= min_items)\n    dataframe = dataframe.groupby(\"userID\").filter(lambda x: len(x) >= min_interactions)\n    return dataframe","metadata":{"id":"t1CgmkQ0Yn1x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = filter_dataset(\n    df,\n    min_items = 1,\n    min_interactions = 4\n)","metadata":{"id":"Vpx01d5eYo5V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_user_rated_item_count(df):\n    user_item_interaction_count = df.groupby(['userID'],as_index = False)['itemID'].count()\n    user_item_interaction_count.rename(columns = {'itemID':'interaction_count'}, inplace = True)\n    return user_item_interaction_count","metadata":{"id":"fg_FVJpBsmVz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interaction_count_df = get_user_rated_item_count(df)\ninteraction_count_df.head()","metadata":{"id":"1uZx88R0dT0S","outputId":"421f4db8-1f88-4570-c30a-2bd9384c1cd1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interaction_count_df.groupby(\"interaction_count\").count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interaction_count_df['interaction_count']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top k items to recommend\nTOP_K = 10\n\n# Model parameters\nLATENT_DIM = 70\nUSER_ENCODER_DIMS = [200,100]\nITEM_ENCODER_DIMS = [500,250,100]\nACT_FUNC = \"elu\"\nLIKELIHOOD = \"bern\"\nNUM_EPOCHS = 1000\nUSER_BATCH_SIZE = 256\nITEM_BATCH_SIZE = 256\n# LEARNING_RATE = 0.001\nUSER_LEARNING_RATE = 0.001\nITEM_LEARNING_RATE = 0.0005\nPLOT_LOSS = False\nBETA_KL = 0.9\n\nbivae = BiVAECF(\n    k=LATENT_DIM,\n    user_encoder_structure = USER_ENCODER_DIMS,\n    item_encoder_structure = ITEM_ENCODER_DIMS,\n    act_fn=ACT_FUNC,\n    likelihood=LIKELIHOOD,\n    n_epochs=NUM_EPOCHS,\n    user_batch_size=USER_BATCH_SIZE,\n    item_batch_size = ITEM_BATCH_SIZE,\n    user_learning_rate=USER_LEARNING_RATE,\n    item_learning_rate=USER_LEARNING_RATE,\n    plot_loss = PLOT_LOSS,\n    seed=69,\n    beta_kl=BETA_KL,\n    use_gpu=torch.cuda.is_available(),\n    verbose=True\n)","metadata":{"id":"7NOURFE2Qpow","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Content based approach","metadata":{"id":"gsmWL8LUts46"}},{"cell_type":"code","source":"cos_sim_data_file = os.path.join(file_path,\"cos_sim.csv\")\ncos_sim_data = pd.read_csv(cos_sim_data_file)","metadata":{"id":"NUe3kek_CoRV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cos_sim_data = cos_sim_data.set_index('symbol').rename_axis('symbol', axis=1)\ncos_sim_data","metadata":{"id":"_lskBNjiFNWg","outputId":"7998071a-e007-4990-b552-a2195a7cad72","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def give_recommendations(stock_business_code,k,print_recommendation):\n    stock_recomm =cos_sim_data.loc[stock_business_code].sort_values(ascending=False).index.tolist()[1:k+1]\n\n    index_recomm = cos_sim_data.loc[stock_business_code].sort_values(ascending=False).values.tolist()[1:k+1]\n\n    result = dict(zip(stock_recomm, index_recomm))\n    # result = {'stocks':stock_recomm,'Index':index_recomm}\n\n    if print_recommendation==True:\n        print('The prefered stock is : {} \\n'.format(stock_business_code))\n        k = 1\n        for stock in stock_recomm:\n            print('The number %i recommended stock is this one: %s \\n'%(k,stock))\n            k = k+1\n    return result","metadata":{"id":"vvKL76evCTDf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_k_recommendations(k,train_):\n    Recommendations = pd.DataFrame(columns=['userID', 'Security', 'itemID','content_prediction'])\n\n    for i, j in train_.iterrows():\n        MASKED_USER_ID = j['userID']\n        Security = j['itemID']\n\n        recomm = give_recommendations(Security,k,False)\n        recommendation_stock = list(recomm.keys())\n        content_prediction=list(recomm.values())\n\n        Recommendations.loc[len(Recommendations.index)] = [MASKED_USER_ID, Security,recommendation_stock, content_prediction]  \n\n    Recommendations_ver2=Recommendations.explode(['itemID', 'content_prediction'])\n    sorted_df = Recommendations_ver2.sort_values(by=['content_prediction'], ascending=False)\n    recommendations_without_duplicates = sorted_df.drop_duplicates(['userID','itemID'], keep='first')\n    Recommendations_top_k = recommendations_without_duplicates.sort_values(['userID', 'content_prediction'], ascending=[True, False]).groupby('userID').head(k)\n\n    del Recommendations_top_k[\"Security\"]\n    Recommendations_top_k['userID'] = Recommendations_top_k['userID'].astype(int)\n    return Recommendations_top_k","metadata":{"id":"k-qV_VOvCL02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross validation for hybrid","metadata":{"id":"XI2AOrVyt4hZ"}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport statistics\nfrom sklearn.model_selection import StratifiedKFold\n\n# Split the data into four folds using stratified KFold\n\ndef cross_val_rec_sys(df, model):\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    NDCGs = []\n    \n    skf = StratifiedKFold(n_splits=4)\n\n    y = df['userID']\n    X = df.drop(['userID'], axis = 1)\n\n    train_sets = []\n    train_s = []\n    test_sets = []\n    test_s = []\n\n    for fold, (train_indices, test_indices) in enumerate(skf.split(X,y)):\n        train_ = df.iloc[train_indices]\n        test_ = df.iloc[test_indices]\n\n        test_ = test_[test_[\"userID\"].isin(train_[\"userID\"].unique())]\n        test_ = test_[test_[\"itemID\"].isin(train_[\"itemID\"].unique())]  \n\n        train_set = Dataset.from_uir(train_.itertuples(index=False), seed= 69)\n        test_set = Dataset.from_uir(test_.itertuples(index=False), seed= 69)\n\n        ndcg_score, results, eval_ndcg_3,eval_ndcg_5,eval_ndcg_15,eval_ndcg_20 = evaluate_recsys(model, train_set, train_, test_)\n        print(ndcg_score)\n        NDCGs.append(ndcg_score)\n\n        train_sets.append(train_set)\n        train_s.append(train_)\n        test_sets.append(test_set)\n        test_s.append(test_)\n    return statistics.mean(NDCGs), NDCGs, train_sets, train_s, test_sets,test_s,results","metadata":{"id":"z_36OhSqOxDo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_final_rating(df):\n    ratings = []\n    for index, row in df.iterrows():\n        userID = row['userID']\n        cf_pred = row['cf_prediction']\n        cb_pred = row['content_prediction']\n        n = interaction_count_df[interaction_count_df['userID']==userID]['interaction_count'].values[0]\n        r = 1 / (1 + np.exp((-n/5)))\n        rating = r*cf_pred + (1-r)*cb_pred\n        ratings.append(rating)\n    return ratings","metadata":{"id":"PClzmvJXd1Ue","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_df(cf_predictions,content_predictions):\n    results = cf_predictions.merge(content_predictions,how='outer')\n    results = results.fillna(0.0)\n    final_ratings = calculate_final_rating(results)\n    results['prediction'] = final_ratings\n    return results","metadata":{"id":"a-3wDfvAp1Gd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_recsys(model, train_set, train_, test_):\n    model.fit(train_set);\n    cb_preds = top_k_recommendations(287, train_)\n    cf_preds = predict_ranking(model, train_, usercol='userID', itemcol='itemID', remove_seen=True)\n    \n    content_predictions = cb_preds.copy()\n    all_predictions = cf_preds.copy()\n    \n    content_predictions['content_prediction'] = MinMaxScaler().fit_transform(np.array(content_predictions['content_prediction']).reshape(-1,1))\n    all_predictions['prediction'] = MinMaxScaler().fit_transform(np.array(all_predictions['prediction']).reshape(-1,1))\n    eval_ndcg_cb = ndcg_at_k(test_, content_predictions, col_user='userID', col_item ='itemID',col_prediction='content_prediction', k=TOP_K)\n    \n    print('eval_ndcg_cb: {}'.format(eval_ndcg_cb))\n    eval_ndcg_cf = ndcg_at_k(test_, all_predictions, col_user='userID', col_item ='itemID',col_prediction='prediction', k=TOP_K)\n    print('eval_ndcg_cf: {}'.format(eval_ndcg_cf))\n    \n    all_predictions.rename(columns = {'prediction':'cf_prediction'}, inplace = True)\n    results = merge_df(all_predictions, content_predictions)\n    \n    eval_ndcg = ndcg_at_k(test_, results, col_user='userID', col_item ='itemID',col_prediction='prediction', k=TOP_K)\n    \n    return eval_ndcg,results","metadata":{"id":"Wd_J1z0vNsd-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hit_rate(model, train_set, train_, test_, k):\n    all_items_indices = train_set.item_ids\n    item_id_to_indices_df = pd.DataFrame()\n    item_id_to_indices_df['itemID'] = train_set.item_ids\n    item_id_to_indices_df['itemIndice'] = train_set.item_indices\n    hits = 0\n    random.seed(0)\n    for user_id in train_set.user_ids:\n\n        user = list(train_set.user_ids).index(user_id)\n\n        #filter out unknown indices\n        train_items = train_[train_['userID'] == user_id]['itemID'].values\n        hold_out_items = test_[test_['userID'] == user_id]['itemID'].values\n        hold_out_items_list = item_id_to_indices_df[item_id_to_indices_df['itemID'].isin(hold_out_items)].itemIndice.values\n        hold_item = random.choice(hold_out_items_list)\n        unknown_items = list(set(all_items_indices) - set(train_items))\n        unknown_items_indices = item_id_to_indices_df[item_id_to_indices_df['itemID'].isin(unknown_items)].itemIndice.values\n\n        #generate recommendations\n        recommended_items = model.rank(user, unknown_items_indices)\n        top_recommendations = list(recommended_items[0])[:k]\n\n        #check if a hit\n        hit = False\n        for item in top_recommendations:\n            if item == hold_item:\n                hit = True\n                break\n        if (hit) :\n            hits += 1\n\n\n    total_users = len(train_set.user_ids)\n\n    hit_rate = (hits/total_users)\n    return hit_rate\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_ndcg, history, train_sets, train_s, test_sets, test_s,results = cross_val_rec_sys(df, bivae)\nprint('avg ndcg : {}'.format(avg_ndcg))\nprint('ndcgs : {}'.format(history))","metadata":{"id":"wS9sv0Q0OLTc","outputId":"0457cd18-3788-44d3-ca47-f1eb9fdc4132","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.to_csv('results.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_sets[3]\ntrain_ = train_s[3]\ntest_ = test_s[3]\nitem_id_to_indices_df = pd.DataFrame()\nitem_id_to_indices_df['itemID'] = train_set.item_ids\nitem_id_to_indices_df['itemIndice'] = train_set.item_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_items_indices = train_set.item_ids\ndef recommender(user_id, k = 10):\n    #grab user indice\n    top_recommendations_list = []\n    user = list(train_set.user_ids).index(user_id)\n\n    #filter out unknown indices\n    train_items = train_[train_['userID'] == user_id]['itemID'].values\n    hold_out_items = test_[test_['userID'] == user_id]['itemID'].values\n    unknown_items = list(set(all_items_indices) - set(train_items))\n    unknown_items_indices = item_id_to_indices_df[item_id_to_indices_df['itemID'].isin(unknown_items)].itemIndice.values\n    \n    #generate recommendations\n    recommended_items = bivae.rank(user, unknown_items_indices)\n    print(recommended_items)\n    top_recommendations = list(recommended_items[0])[:k]\n    \n    for item in top_recommendations:\n        top_recommendations_list.append(item_id_to_indices_df[item_id_to_indices_df['itemIndice']==item]['itemID'].values[0])\n    return top_recommendations_list","metadata":{"id":"YMDz170QbEsy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final recommendations","metadata":{}},{"cell_type":"code","source":"def recommend_cb(userID, k):\n    itemIDs = df_all[df_all['userID']==userID]['itemID'].values.tolist()\n    cb_recommendations = {}\n    for i in itemIDs:\n        stock_recomm =cos_sim_data.loc[i].sort_values(ascending=False).index.tolist()[1:k+1]\n        index_recomm = cos_sim_data.loc[i].sort_values(ascending=False).values.tolist()[1:k+1]\n        result = dict(zip(stock_recomm, index_recomm))\n        cb_recommendations.update(result)\n\n    cf_results = remove_duplicates(cb_recommendations)\n    cb_recommendations_sorted = dict( sorted(cf_results.items(), key=operator.itemgetter(1),reverse=True))\n    final_recommendations = list(cb_recommendations_sorted.keys())[0:k]\n    return final_recommendations","metadata":{"id":"1V-b4dYTly-8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_duplicates(rec_dict):\n    result = {}\n    for key,value in rec_dict.items():\n        if value not in result.values():\n            result[key] = value\n    return result","metadata":{"id":"V208sSwYmMVG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sec_gics = os.path.join(file_path,\"fullcompanylist.csv\")\n\nsecurity_file_withgics = pd.read_csv(sec_gics)\nsecurity_file_withgics = security_file_withgics.drop(['Unnamed: 0', 'Unnamed: 0.1', 'buisnesssummary', 'address'], axis=1)\nfor i in range(security_file_withgics.shape[0]):\n    security_file_withgics = security_file_withgics.replace(security_file_withgics['symbol'][i],security_file_withgics['symbol'][i].strip('()'))\nsecurity_file_withgics['symbol'] = security_file_withgics['symbol'].apply(lambda x : x.split(\".\")[0])\nsecurity_file_withgics = security_file_withgics.drop_duplicates()\nsecurity_file_withgics = security_file_withgics.reset_index(drop=True)\nsecurity_file_withgics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def view_recommendation(recommendations):\n    rec_list = []\n    for i in recommendations:\n        rec_tuple = tuple(security_file_withgics[security_file_withgics['symbol']==i].values[0])\n        rec_list.append(rec_tuple)\n    df_recommedations = pd.DataFrame(rec_list, columns=['name', 'symbol', 'gics'])  \n    return df_recommedations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recommend_hybrid(userID,k):\n    recommendations = []\n    print('Top {} recommended stocks for user {}'.format(k,userID))\n    if (userID in df['userID'].values):\n        print('Recommendationa from weighted hybrid system:')\n        return view_recommendation(recommender(userID, k))\n    else:\n        print('Recommendationa from content based system:')\n        return view_recommendation(recommend_cb(userID, k))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_recommendations = recommend_hybrid(3,10)\nfinal_recommendations","metadata":{},"execution_count":null,"outputs":[]}]}
