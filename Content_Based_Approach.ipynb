{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebzCpUi7sN3f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Lambda, Dense, Dropout, Input, Layer, TextVectorization, concatenate\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "import math\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from tensorflow.python.ops import nn\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "from math import log\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial import distance\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from keras.utils import plot_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k85yEvC6gnIu"
      },
      "outputs": [],
      "source": [
        "# Stock data is the dataset which contains the details of stocks. Main features: stock symbol, buisness name, buisness summary, GICS code\n",
        "stock_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5JzSxiVlxae"
      },
      "outputs": [],
      "source": [
        "stock_data = stock_data.fillna('unknown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyLyZReB-XWt"
      },
      "source": [
        "### Clean buisness summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4uFXHv1-XWu"
      },
      "outputs": [],
      "source": [
        "def clean_text(\n",
        "    string: str,\n",
        "    ) -> str:\n",
        "\n",
        "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
        "    stop_words=['the', 'a', 'and', 'is', 'be', 'will','&','of','for','are']\n",
        "\n",
        "    \"\"\"\n",
        "    A method to clean text\n",
        "    \"\"\"\n",
        "\n",
        "    # Removing the punctuations\n",
        "    for x in string.lower():\n",
        "        if x in punctuations:\n",
        "            string = string.replace(x, \"\")\n",
        "\n",
        "    # Converting the text to lower\n",
        "    string = string.lower()\n",
        "\n",
        "    # Removing stop words\n",
        "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
        "\n",
        "\n",
        "    # stemming\n",
        "    snowball = SnowballStemmer(language='english')\n",
        "\n",
        "    str_list=string.split()\n",
        "    newStr=''\n",
        "    for i in str_list:\n",
        "        newStr = newStr+snowball.stem(i)+' '\n",
        "\n",
        "\n",
        "    return newStr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl0jhAN5-XWw"
      },
      "outputs": [],
      "source": [
        "stock_data.buisnesssummary = stock_data.buisnesssummary.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cd6ROh0he2_"
      },
      "outputs": [],
      "source": [
        "stock_data['symbol'] = stock_data['symbol'].str.replace(r\"(\",\"\")\n",
        "stock_data['symbol'] = stock_data['symbol'].str.replace(r\")\",\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQLc8Wj52Sg5"
      },
      "outputs": [],
      "source": [
        "#Vectorizer to embed text data\n",
        "\n",
        "vectorizer = HashingVectorizer(n_features=5000,norm=None,alternate_sign=False)\n",
        "vectorized_sentenses = vectorizer.fit_transform(text_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used this function to convert both buisness name and summary in a row to vector using vectorizer\n",
        "def row_to_vector(dataframe, text_columns, id_column):\n",
        "\n",
        "  output_dataframe = pd.DataFrame(columns = [id_column,'vector'])\n",
        "  column_list = dataframe.columns.to_list()\n",
        "  text_column_pos = []\n",
        "\n",
        "  for column in text_columns:\n",
        "    text_column_pos.append(column_list.index(column))\n",
        "\n",
        "  text_column_pos.sort(reverse=True)\n",
        "\n",
        "  for i in range(dataframe.shape[0]):\n",
        "    row = (dataframe.iloc[i].to_list())[0:3]\n",
        "    row_id = row[(column_list.index(id_column))]\n",
        "\n",
        "    for pos in text_column_pos:\n",
        "      text = row[pos]\n",
        "      text_vector = vectorizer.transform([text]).toarray().flatten().tolist()\n",
        "      row = row[:pos] + text_vector + row[pos+1:]\n",
        "    row.remove(row_id)\n",
        "    output_dataframe.loc[i] = [row_id, row]\n",
        "  return output_dataframe"
      ],
      "metadata": {
        "id": "snam4cyc_ssh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_vectors = row_to_vector(stock_data, ['name','buisnesssummary'], 'symbol')"
      ],
      "metadata": {
        "id": "H17WX5622_71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Auto Encoder"
      ],
      "metadata": {
        "id": "BIM-gHfINDk9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsmEtqi-hU90"
      },
      "outputs": [],
      "source": [
        "input_shape = np.array(stock_data_vectors['vector'][0]).shape\n",
        "print('vector size(hashing vectorized name + hashing vectorized buisness summary + one hot encoded gics codes):',input_shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbt0Xxj9ywzW"
      },
      "outputs": [],
      "source": [
        "# input_dim = 2333\n",
        "latent_dim = 50\n",
        "\n",
        "epochs = 20\n",
        "decay = 1e-4\n",
        "bias = True\n",
        "\n",
        "\n",
        "input_dim = input_shape[0]\n",
        "# input_dim = len(stock_data_vectors['vector'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO7CpRNFs7V8"
      },
      "outputs": [],
      "source": [
        "#Encoder (Functional model)\n",
        "encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
        "\n",
        "encoder_layer1 = Dense(2048, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=bias, activation='relu')(encoder_input)\n",
        "\n",
        "encoder_layer2 = Dense(512, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=bias, activation='relu')(encoder_layer1)\n",
        "\n",
        "encoder_layer3 = Dense(128, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=bias, activation='relu')(encoder_layer2)\n",
        "\n",
        "z_mean = Dense(latent_dim, name = 'z_mean')(encoder_layer3)\n",
        "z_log_var = Dense(latent_dim, name = 'z_log_variance')(encoder_layer3)\n",
        "\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(encoder_input, [z_mean,z_log_var, z], name = 'encoder')\n",
        "encoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAI7KlQ6xB2T"
      },
      "outputs": [],
      "source": [
        "#Decoder\n",
        "\n",
        "decoder_input = Input(shape = (latent_dim,), name = 'decoder_input')\n",
        "decoder_layer1 = Dense(128, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=bias, activation='relu')(decoder_input)\n",
        "\n",
        "decoder_layer2 = Dense(512, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=bias, activation='relu')(decoder_layer1)\n",
        "\n",
        "decoder_layer3 = Dense(2048, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=bias, activation='relu')(decoder_layer2)\n",
        "\n",
        "decoder_output = Dense(input_dim,  activation = 'sigmoid')(decoder_layer3)\n",
        "\n",
        "decoder = Model(decoder_input, decoder_output, name = 'decoder')\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(encoder, show_layer_names = False, show_shapes = True, dpi = 60)"
      ],
      "metadata": {
        "id": "y3x0ZWC9BMU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY434c5t-XW-"
      },
      "outputs": [],
      "source": [
        "plot_model(decoder, show_layer_names = False, show_shapes = True, dpi = 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaEl-8Dl4cvR"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        _,_,z = self.encoder(inputs)\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"total_loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUVDIEMOWCF2"
      },
      "outputs": [],
      "source": [
        "train_data = (stock_data_vectors['vector'].values).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYauGNzNXdXj"
      },
      "outputs": [],
      "source": [
        "train_data_arr = np.array(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbCJ-aFSWyvd"
      },
      "outputs": [],
      "source": [
        "# train_data_tensor = tf.convert_to_tensor(train_data_arr, dtype=tf.int64)\n",
        "train_data_tensor = np.asarray(train_data_arr).astype(np.int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsiuWtlh9riB"
      },
      "outputs": [],
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = vae.fit(train_data_arr, epochs=15, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "U54OnEmH_DdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YnPZVUjlKbh"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = vae.encoder.predict(train_data_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afWzQh_2tDCK"
      },
      "outputs": [],
      "source": [
        "encoded_vectors_text = encoder_outputs[-1].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ///weight calculation for GICS code\n",
        "\n",
        "mx=[]\n",
        "mn=[]\n",
        "for i in range(0,len(encoded_vectors_text)):\n",
        "  mx.append(sorted(encoded_vectors_text[i])[-1])\n",
        "  mn.append(sorted(encoded_vectors_text[i])[0])\n",
        "min_value = sorted(mn)[0]\n",
        "mx.append(-1*min_value)\n",
        "\n",
        "max_value = sorted(mx)[-1]\n",
        "weight = math.ceil(max_value)\n",
        "\n",
        "weight"
      ],
      "metadata": {
        "id": "e2jTW5RyGjLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final vector for a stock\n",
        "row = []\n",
        "for i in range(stock_data.shape[0]):\n",
        "    gics=((stock_data.iloc[i].to_list())[3:])\n",
        "    weighted_gics = [i * weight for i in gics]\n",
        "    row.append(encoded_vectors_text[i]+weighted_gics)\n"
      ],
      "metadata": {
        "id": "Z7zUNu-kAquw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj-4j0k_VtkB"
      },
      "outputs": [],
      "source": [
        "size=len(stock_data_vectors.iloc[0].at['encoded_vectors'])\n",
        "\n",
        "def reshape_vectors(vector):\n",
        "  arr = np.array(vector).reshape(1,size)\n",
        "  return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSvirmgWV9RI"
      },
      "outputs": [],
      "source": [
        "stock_data_vectors['encoded_vectors'] = stock_data_vectors['encoded_vectors'].apply(lambda x : reshape_vectors(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l91C-T6p0Pus"
      },
      "outputs": [],
      "source": [
        "stock_data_vectors['symbol'] = stock_data_vectors['symbol'].apply(lambda x : x.split(\".\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_vectors = row\n",
        "len(encoded_vectors)"
      ],
      "metadata": {
        "id": "hoErJIm0Qodh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl8zUZR-e4lE"
      },
      "outputs": [],
      "source": [
        "# Calculating Cosine Similarity between Stocks\n",
        "cos_sim_data = pd.DataFrame(cosine_similarity(encoded_vectors), index = stock_data_vectors['symbol'], columns = stock_data_vectors['symbol'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aco0fwE1f_B1"
      },
      "outputs": [],
      "source": [
        "# Generating recommendations using cosine similarity matrix\n",
        "# stock_business_code: stock symbol, k: number of recommendation, needed to generated, print_recommendation: need to pring recommendations\n",
        "\n",
        "def give_recommendations(stock_business_code,k,print_recommendation):\n",
        "  stock_recomm =cos_sim_data.loc[stock_business_code].sort_values(ascending=False).index.tolist()[1:k+1]\n",
        "  index_recomm = cos_sim_data.loc[stock_business_code].sort_values(ascending=False).values.tolist()[1:k+1]\n",
        "\n",
        "  result = dict(zip(stock_recomm, index_recomm))\n",
        "\n",
        "  if print_recommendation==True:\n",
        "    print('The prefered stock is : {} \\n'.format(stock_business_code))\n",
        "    k = 1\n",
        "    for stock in stock_recomm:\n",
        "      print('The number %i recommended stock is this one: %s \\n'%(k,stock))\n",
        "      k = k+1\n",
        "\n",
        "  return result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}